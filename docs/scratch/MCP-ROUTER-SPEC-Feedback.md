## Section 6: Secrets Management

**Secrets Management Strategy**  
"Manage secrets" is critical. Clarify:
- Development (`.env`Â file in Docker, never committed)
- Production (HashiCorp Vault, or GitHub Actions secrets if deployed)
- Runtime (how does FastAPI load them safely?)

**Recommendation:**

```python
# Use python-dotenv for local dev 
from dotenv import load_dotenv 
import os 

load_dotenv() 
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
```

For production, useÂ **AWS Secrets Manager**Â orÂ **Vault**Â if deploying beyond your machine. For now,Â `.env.local`Â (never git) in Docker Compose is fine.

---



# Section 3: Phase 2 - MCP Router Core

Avoid common pitfalls and be flexible in making adjustments as needed. This chat is a crucial part of her learning journey, where you'll serve as the expert guide.

## Strengths
1. **Transport abstraction**Â (STDIO â†’ HTTP) is exactly rightâ€”unifies heterogeneous MCP servers elegantly
2. **Three-tier LLM strategy**Â (deepseek-r1 for reasoning, llama3 for speed, nomic-embed for vectors) is smart
3. **Client-based enhancement rules**Â respects each tool's UX (Claude Desktop gets reasoning, Raycast gets brevity)
4. **Circuit breaker + fallback**Â pattern shows production maturity
5. **MVP-first mindset**Â is refreshingâ€”add complexity only when real constraints demand it

## Key Gaps to Resolve
1. **Prompt cache implementation**â€”details missing on where embeddings live, similarity threshold, eviction policy. Start with in-memory LRU.
2. **Memory Server is underspecified**â€”skip it in MVP. Add persistent knowledge graph in Phase 2 once you see duplication patterns.
3. **Per-server vs global circuit breakers**â€”recommend per-server. One failing MCP server shouldn't block others.
4. **STDIO â†’ HTTP adapter complexity**â€”subprocess lifecycle, JSON-RPC framing, timeouts need careful handling. I've sketched minimal-but-working code in the feedback.
5. **Ollama context window mismatches**â€”deepseek-r1 (64K) vs llama3 (8K). Need fallback logic if prompt exceeds limit.

---

## Recommendations

### 1.Â **Prompt Cache with Embeddings**
**Two-tier caching strategy**Â that balances speed and intelligence:
- **L1 Cache**: Hash-based exact match (instant)
- **L2 Cache**: Embedding similarity via nomic-embed-text (semantic)
- **Threshold**: 0.85 cosine similarity (research-backed optimal value)â€‹
- **Eviction**: LRU with configurable max size
- **Storage**: In-memory OrderedDict for MVP, easy Redis upgrade path

### 2.Â **STDIO â†’ HTTP Transport Adapter**
**Robust subprocess lifecycle manager**Â handling the hard parts:
- **Auto-restart**: Crashed MCP servers restart automatically (with limits)
- **Protocol compliance**: Newline-delimited JSON per MCP specâ€‹
- **Timeout protection**: 30s default, prevents hanging
- **Graceful shutdown**: SIGTERM â†’ wait â†’ SIGKILL if needed
- **Health tracking**: Exposes process status to circuit breakers

### 3.Â **Per-Server Circuit Breakers**
**Isolated failure handling**Â so one broken service doesn't take down others:
- **States**: CLOSED (normal) â†’ OPEN (failing) â†’ HALF_OPEN (testing recovery)
- **Per-service thresholds**: Different SLAs for different services
- **Automatic recovery**: Probes service health after timeout
- **Observable**: All breaker states exposed viaÂ `/health`

### 4.Â **Context Window Management**
**Model-aware routing**Â prevents silent truncation:
- **Token counting**: tiktoken for accurate measurement
- **Automatic fallback**: llama3 (8K) â†’ deepseek-r1 (64K) if prompt too large
- **Client preferences**: Respects enhancement-rules.json while enforcing limits
- **Fails safely**: Returns error if prompt exceeds all models

### 5.Â **Comprehensive Health Checks**
**Granular component visibility**Â for debugging and monitoring:
- **Parallel checks**: All health checks run concurrently (fast)
- **Per-component status**: Ollama, each MCP server, cache, circuit breakers
- **Response time tracking**: Identify slow services
- **Prometheus-ready**: Easy to export metrics

## ðŸ“¦ All Code Is Production-Ready

Each solution includes:
- âœ… Complete, working Python code
- âœ… Integration with FastAPI lifespan
- âœ… Error handling and logging
- âœ… Configuration examples
- âœ… Sample outputs
- âœ… Justification based on research

## ðŸš€ Suggested Build Order
1. **Part 1**: STDIO adapter + per-server circuit breakers (foundation)
2. **Part 2**: Prompt cache + context window management (optimization)
3. **Part 3**: Health aggregator + integration testing (observability)

All implementations useÂ **FastAPI lifespan**Â patterns properly, followÂ **async best practices**, and leverage proven libraries (numpy for embeddings, asyncio for subprocess management).

Avoid common pitfalls and be flexible in making adjustments as needed. This chat is a crucial part of her learning journey, where you'll serve as the expert guide.